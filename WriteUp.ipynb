{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity -- Self Driving Car Nano Degree\n",
    "# Behavioral Cloning\n",
    "\n",
    "### John Mansell\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Included Files\n",
    "> [model.py](model.py) -- The script to create and train the model  \n",
    "> [drive.py](drive.py) -- Drives the car in autonomous mode  \n",
    "> [model.h5](model.h5) -- The trained convolutional Neural Network  \n",
    "> [Write up](WriteUp.ipynb) -- Write Up for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning\n",
    "> This project focusses on behavioral cloning. The idea is to expose the computer to a variety of driving conditions as well as the \"correct action\" for that particular situation. \n",
    "\n",
    "> For example, in our project, a human drives around a track. A camera at the front of the car records the track and the current steering angle. Then, all that data is fed to a neural network. The computer then searches for patterns. The goal is to produce a model where the computer can predict the correct steering angle given an input image. If it can correctly predict the steering angle, then the computer can safely drive the car arround the track.\n",
    "\n",
    "> This approach is known as behavioral cloning because we don't program rules into the algorithm. Rather, the computer analyzes human behavior and tries to replicate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering the Data\n",
    "> The first step was to gather the data. Using the simulator provided by Udacity, I drove the car in \"Training Mode\" and recorded data. The simulator saves images taken from a simulated camera in the front of the car and records the steering angle. Below is an example of the recoded images.\n",
    "![](writeup_images/road_1_angle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Images\n",
    "> Before passing the images to the nerual network for training, it helps to preprocess the image. The goal is to create a pipeline which will isolate the important information so that the neural network will be trained on the relevant information, not on extranious data.\n",
    "\n",
    "> ### Image Mask\n",
    "> One useful technique is to crop the frame.\n",
    "\n",
    "> The top half of the image is mostly sky. As humans, we know this isn't relevant to the curvature of the road, but the neural network is learning from scratch what patterns to look for in the image. It helps to remove parts of the image which aren't pertanant to driving.\n",
    ">![](writeup_images/Masked2.png)\n",
    "\n",
    "> ### Color Space\n",
    "> I also tried converting the images into different color spaces. This can make certain parts of the image stand out. Of all the color spaces, I found LAB and HSV to be the most useful in creating accurate neural network models.\n",
    "> #### LAB\n",
    ">![](writeup_images/LAB.png)\n",
    "> #### HSV\n",
    ">![](writeup_images/HSV.png)\n",
    "\n",
    "> ### Normalize Brightness\n",
    "> Different parts of the road can be significantly brighter than others. This can create a false positive when the neural network is trying to identify the road markers. I dealt with this by normalizing the brightness of each image. \n",
    "\n",
    "> I achieved this by separating the HSV image into H,S,V channels. Then I normalized each image by dividing the image by np.max(V). This was an effective way of normalizing the brightness of all the images around the track\n",
    "\n",
    "> ### Warping the Image\n",
    "> I pulled a technique from the \"Advanced Lane Finding\" project. I warped the perspective of the image to be top-down. This was an effective way of feeding only the relevant information of the road into the neural network for training.\n",
    "![](writeup_images/Warped.png)\n",
    "\n",
    "> ### Mag Threshold\n",
    "> Once the image was converted into a top-down perspective, I utilized another technique from the \"Advanced Lane Finding\" project. I applied a bilatteral filter, and then filtered the image by magnitude of the gradient.\n",
    "![](writeup_images/final.png)\n",
    "\n",
    "> ## Pipeline\n",
    "> While I tried different techniques, the pipeline that gave the best results was to use the techniques from the \"Advanced Lane Findnig\" project. My final image processing pipeline was:\n",
    "> * convert from BGR to RGB\n",
    "> * Warp perspective\n",
    "> * Bilateral Filter\n",
    "> * Filter by magnitude of gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the Data Set\n",
    "> Driving around the track, most of the steering angles recorded will be straight. This can lead the neural network to conclude that the best approach is to drive straight all the time. To counteract this, I balanced the data set. That is, I duplicated some of the images which correspond to steering angles which are not zero so that the network gets more used to seeing those examples. This helps to not give the network a bias towards a zero steering angle.\n",
    "\n",
    "> Another way of balancing the dataset was to give the model more examples of turning left and right. I did this by using the right and left camera images and then added a correction factor to the steering angle. Effectively, this simulated driving around the track slightly closer to the edge, and steering away from the edge.\n",
    "\n",
    "> Finally, I also flipped each image and measurement in the data set. This had the effect of simulating driving around the track in the oposite direction.\n",
    "\n",
    "> Below is a histogram of the data set before and after balancing the data. \n",
    "    <img src=\"./writeup_images/hist_1.png\" width=\"400\", height=\"200\"/> \n",
    "    <img src=\"./writeup_images/hist_2.png\" width=\"400\", height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "> After processing the images and balancing the data set, I passed the images and their measurements to the neural network for training.\n",
    "\n",
    "> #### Over Fitting\n",
    "> To avoid overfitting, I split the dataset into a training and validation set. In this project, the \"test\" set was testing the model driving around the track in autonomous mode. The validaton set was 20% of the final images.\n",
    "\n",
    "> #### NN Model\n",
    "> The model that I used is based on the [NVidia end to end model](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf). I removed a few of the layers for computational efficiency. I originally started with the LeNet architecture, but wasn't able to get successful results. I read up on some of the other students projects and comments in the slack channel and the project forums. More than one student had found success using this model, so I gave it a try and found it to be very robust.\n",
    "\n",
    "> The final model in my project (model.py :: lines 227 - 245) consisted of :\n",
    "* a lambda layer for normalizing the data.\n",
    "* Three 5x5 convolutions\n",
    "* Two 3x3 convolutions\n",
    "* Three fully connected layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Output\n",
    "> Once the model was trained, I used the drive.py script provided by Udacity to drive the car around the track in autonomous mode. [run1.mp4](run1.mp4) is a video of a successful autonomous lap around the track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons Learned\n",
    "> ### SANITY CHECK ALL YOUR DATA!\n",
    "> In this project I learned the value of sanity checking all the data. Multiple times I would make a change to the preprocessing pipeline, but that change wasn't making it all the way to the neural network. For example, preprocessing images doesn't always happen \"in place\" so I had to create multiple arrays. Also, the model.py script reads images in through openCV which uses the BGR color space, but drive.py expects images in RGB color space. At other times, changes were being applied to either myData, or the UdacityData, but not both.\n",
    "\n",
    "> I started building in checks that would check lenghts of arrays such that the script would sanity check that all the appropriate image processing steps had been taken. I also would show one of the images just before the data is passed to the neural network so that I can visually see that the process I intended to happen actually happened. This helped me multiple times when the images were in the wrong color space, or weren't what I expected them to be.\n",
    "\n",
    "> #### Its all about the data\n",
    "> One lesson I learned from reading through the forums and other students projects is that its all about the data. The quality and quantity of images going in has the greatest effect on the strength of the model. If the network hasn't seen enough examples of what to do in each situation, it can't predict the correct action or steering angle.\n",
    "\n",
    "> #### Pickle the data\n",
    "> Pickling the data allowed for much faster tests of changes to the pipeline. By pickling the data, I was able to read in the images and measurements at different parts of the pipeline much more quickly. If I made a change to the pipeline, I could read them in from scratch, but otherwise it saved a lot of time to pickle save and load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Improvements\n",
    "> A more robust pipeline would help the model to deal with more varied situations, such as the challenge track. If I were visiting this again, I would add in more controls to get rid of visual noise in the preprocessing steps. This could be done through color thresholds, or a more robust gradient filter.\n",
    "\n",
    "> More training data would also help to create a more robust pipeline. Since I had computational limitations with my set up. I limited the data to only two laps around the track. That afforded me a pretty quick turn around on testing different ideas, but didn't create the most robust model.\n",
    "\n",
    "> In reality, an actual car would need thousands of miles on various roads to really start to build a robust model for self driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "> I couldn't have done this project on my own. The udacity lessons were invaluable and much of the starting functionality was taken directly from the lessons, or from their video walk through of the project.\n",
    "\n",
    "> I also read through some of the other students projects for ideas on what they had found usefull in areas such as the model architechture or color space they used. specifically, [Jeremy Shannon's](https://github.com/jeremy-shannon/CarND-Behavioral-Cloning-Project) pointed me in the direction of NVidia's neural network architechture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
